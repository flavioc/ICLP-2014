
\newcommand{\mz}{\m{match} \;}
\newcommand{\tab}[0]{\;\;\;\;}
\newcommand{\dz}{\m{derive} \;}
\newcommand{\comp}[0]{\m{comp} \;}
\newcommand{\az}{\m{apply} \;}
\newcommand{\doz}{\m{run} \;}
\newcommand{\seqnocut}[3]{#1 ; #2 \Rightarrow #3}
\newcommand{\defeq}{\buildrel\triangle\over =}
\newcommand{\compr}[1]{\m{def} \; #1}

\newcommand{\mo}{\m{match}_{LLD} \;}
\newcommand{\cont}{\m{cont}_{LLD} \;}
\newcommand{\contc}{\m{cont}_{LLDc} \;}
\newcommand{\done}{\m{derive}_{LLD} \;}
\newcommand{\doo}{\m{run}_{LLD} \;}
\newcommand{\mc}[0]{\m{match}_{LLDc} \; }
\newcommand{\dall}[0]{\m{fix}_{LLD} \; }
\newcommand{\strans}[0]{\m{update}_{LLD} \;}
\newcommand{\dc}{\m{derive}_{LLDc} \;}
\newcommand{\ao}{\m{apply}_{LLD} \;}

We now present the sequent calculus of a fragment of intuitionistic linear
logic~\cite{girard-87} used by LM followed by the dynamic semantics built on top of
this fragment.

The fragment of linear logic used by LM is presented in \ref{linear_logic}.
We use a standard set of connectives except the $\m{def} \; A$ connective, which is used to logically justify comprehensions and aggregates.
The sequent calculus has the form $\Psi; \Gamma; \Delta \rightarrow C$, where $\Psi$ is the typed term context used in the quantifiers, $\Gamma$ is the set
of persistent terms, $\Delta$ is the multi-set of linear propositions and $C$ is the proposition we want to prove.
Table~\ref{table:linear} relates the linear logic directives with the LM syntax.


\footnotesize
\begin{center}
\begin{table}\resizebox{14cm}{!}{
    \begin{tabular}{ c l c l}
    \hline
    Connective                   & Description                                        & LM Place   & LM Example                           \\ \hline \hline
    $\emph{fact}(\hat{x})$       & Linear facts.                                      & Body/Head  & \texttt{path(A, P)}                  \\ 
    $\bang \emph{fact}(\hat{x})$ & Persistent facts.                                  & Body/Head  & \texttt{$\bang$edge(X, Y, W)}        \\ 
    $1$                          & Represents rules with an empty head.               & Head       & \texttt{1}                           \\ 
    $A \otimes B$                & Connect two expressions.                           & Body/Head  & \texttt{p(A), e(A, B)}               \\ 
    $\forall x. A$               & For variables defined in the rule.                 & Rule       & \texttt{p(A) $\lolli$ r(A)}          \\ 
    $\exists x. A$               & Instantiates new node variables.                   & Head       & \texttt{exists A.(p(A, P))}          \\ 
    $A \lolli B$                 & $\lolli$ means "linearly implies".                 & Rule       & \texttt{p(A, B) $\lolli$ r(A, B)}    \\ 
                                 & $A$ is the body and $B$ is the head.               &            &                                      \\ 
    $\m{def} A. B$               & Extension called definitions. Used for             & Head       & \texttt{\{B | !e(A, B) | v(B)\}}     \\ 
                                 & comprehensions and aggregates.                     &            &                                      \\ \hline
    \end{tabular}}
\caption{Connectives from Linear Logic used in LM.}
\label{table:linear}
\end{table}
\end{center}
\normalsize


For definitions, we took inspiration from Baelde's work on least and greatest fixed points in linear logic~\cite{Baelde:2012:LGF:2071368.2071370}. Baelde's system goes beyond
simple recursive definitions and allows proofs using induction and co-induction in linear logic. Fortunately, the proof system satisfies
cut-elimination~\cite{Baelde:2012:LGF:2071368.2071370} which makes it consistent.

In a comprehension, we want to apply an implication to as many matches as the database allows. One way to formally describe comprehensions would be to use persistent
rules that would be used a few times and then would be forgotten. A more reasonable approach is to use
definitions. Given a comprehension $C = \{ \; \widehat{x}; \; A; \; B \; \}$ with a body $A$ and a head $B$, then we can build the following recursive definition:

\[
\compr{C} \defeq 1 \with ((A \lolli B) \otimes \compr{C})
\]

We can unfold $\compr{C}$ to either stop (by selecting $1$) or get a new linear implication $A \lolli B$
to apply the comprehension once. This uses linear logic's additive conjunction $\with$.
Because we also get another $\compr{C}$ by selecting the right hand side,
the comprehension can be applied again, recursively.

This form of definition does not capture the desired \emph{maximality} aspect of the comprehension,
since it commits to finding a particular form of proof and not all possible proofs. The low level
operational semantics will ensure maximality.

Aggregates work identically, but they need an extra argument to accumulate the aggregated value. If a sum aggregate $C$ has the form $[\;\m{sum} \Rightarrow y; \; \widehat{x}; \; A; \; B_1; \; B_2 \;]$, then the definition will be as follows (the aggregate is initiated as $\compr{C} \; 0$):

\[
\compr{C} \; V \defeq (\lambda v. B_2)V \with (\forall x. ((Ax \lolli B_1) \otimes \compr{C} \; (x + V)))
\]

\subsection{Dynamic Semantics}

The high level dynamic semantics (HLD) formalize the mechanism of matching and deriving a single rule. The semantics receive the database and the program's rules as inputs and return as outputs the consumed linear facts, derived linear facts and derived persistent facts. Then, it is possible
to compute the program as a sequence of steps, until no more rules are applicable.
 
The HLD semantics are closely related to the linear logic fragment presented above.
From the sequent calculus, we consider $\Gamma$ and $\Delta$ the database of persistent and linear facts, respectively.
We consider the rules of the program as persistent linear implications of the form $\bang (A \lolli B)$ that we put in a separate context $\Phi$.
We ignore the right hand side $C$ of the calculus and use inversion on the $\Delta$ and $\Gamma$ contexts so that we only have atomic terms (facts). To apply rules
we use chaining by focusing on the derivation rules of $\Phi$.

\scriptsize
\begin{figure}[h]

\[
\infer[\doz rule]
{\doz \Gamma ; \Delta ; R, \Phi \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\az \Gamma ; \Delta ; R \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\az rule]
{\az \Gamma ; \Delta, \Delta'' ; A \lolli B \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\mz \Gamma ; \Delta \rightarrow A & \dz \Gamma ; \Delta''; \Delta; \cdot ; \cdot ; B \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\mz 1]
{\mz \Gamma; \cdot \rightarrow 1}
{}
\tab
\infer[\mz p]
{\mz \Gamma; p \rightarrow p }
{}
\tab
\infer[\mz \bang p]
{\mz \Gamma, p; \cdot \rightarrow \bang p}
{}
\]

\[
\infer[\mz \otimes]
{\mz \Gamma; \Delta_1, \Delta_2 \rightarrow A \otimes B}
{\mz \Gamma; \Delta_1 \rightarrow A & \mz \Delta_2 \rightarrow B}
\]

\[
\infer[\dz p]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; p, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; p, \Delta_1 ; \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\tab
\infer[\dz \bang p]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; \bang p, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1, p ; \Delta_1 ; \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz \otimes]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A \otimes B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A, B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\tab
\infer[\dz 1]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1; \Delta_1 ; 1, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1; \Delta_1 ; \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz end]
{\dz \Gamma ; \Delta ; \Xi' ; \Gamma' ; \Delta' ; \cdot \rightarrow \Xi' ; \Delta' ; \Gamma'}
{}
\]


\[
\infer[\dz comp]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; \comp A \lolli B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; 1 \with (A \lolli B \otimes \comp A \lolli B), \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz \lolli]
{\dz \Gamma ; \Delta_a, \Delta_b ; \Xi ; \Gamma_1 ; \Delta_1 ; A \lolli B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\mz \Gamma ; \Delta_a \rightarrow A & \dz \Gamma ; \Delta_b ; \Xi, \Delta_a ; \Gamma_1 ; \Delta_1 ; B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz \with L]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A \with B, \Omega \rightarrow \Xi' ; \Delta'; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A, \Omega \rightarrow \Xi' ; \Delta'; \Gamma'}
\tab
\infer[\dz \with R]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A \with B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\caption{High Level Dynamic Semantics.}
\label{hld_semantics}
\end{figure}
\normalsize

The HLD semantics are shown in Fig.~\ref{hld_semantics} and are composed of four judgments:
\begin{enumerate}
   \item $\doz \Gamma; \Delta; \Phi \rightarrow \Xi'; \Delta'; \Gamma'$ picks a rule from $\Phi$ and applies it using facts from $\Gamma$ and $\Delta$. $\Xi'$, $\Delta'$ and $\Gamma'$ are the outputs of the derivation process. $\Xi'$ are the linear facts consumed, $\Delta'$ are the linear facts derived and $\Gamma'$ the new persistent facts;
   \item $\az \Gamma; \Delta; R \rightarrow \Xi'; \Delta'; \Gamma'$ picks a subset of linear facts from $\Delta$ and matches the body of the rule $R$ and then derives the head;
   \item $\mz \Gamma; \Delta \rightarrow A$ verifies that $\Delta$ proves $A$ directly;
   \item $\dz \Gamma; \Delta; \Xi; \Gamma_1; \Delta_1, \Omega \rightarrow \Xi'; \Delta'; \Gamma'$ deconstructs and instantiates the ordered head terms $\Omega$ and adds them to $\Delta_1$ and $\Gamma_1$, the contexts for new linear and persistent facts, respectively.
\end{enumerate}

Comprehensions are derived by non-deterministically deciding to apply the comprehension ($\dz \with L$ and $\dz \with R$)
and then using the $\mz$judgment in the rule $\dz comp$. We note that the HLD semantics do not take distribution into account,
since we assume that the database is global. We also omit the quantifiers and do not deal with unification since it complicates the formal system.

The low level dynamic semantics (LLD) shown in \ref{low_level_semantics} improve upon HLD by adding rule priorities, by removing non-determinism
when matching the body of rules by modeling all the matching steps and by applying comprehensions or aggregates as many times as the database allows.
Selectors can also be trivially implemented in LLD, although they are not shown in paper.

In LLD we try all the rules in order. For each rule, we use a \emph{continuation stack} to store the \emph{continuation frames} created by
each fact template $p$ present
in the body of the rule. Each frame considers all the facts relevant to the template given the current variable bindings ($\mo$rules), that
may or not fail during the remaining matching process. If we fail, we backtrack to try other alternatives (through $\cont$rules). If the
continuation stack becomes empty, we backtrack to try the next rule (rule $\cont next \; rule$). When we succeed the facts consumed are known
($\mo end$).

The derivation process in LLD is similar to the one used in HDL, except for the case of comprehensions or aggregates. For such cases
($\done comp$), we need to create a continuation stack and start matching the body of the expression as we did before.
When we match the body ($\mc$judgment), we fully derive the head ($\dc$judgment) and then we reuse the continuation stack to find which
other combinations of the database facts can be consumed ($\dc end$). By definition, the continuation stack contains
enough information to go through all combinations in the database.

However, in order to reuse the stack, we need to \emph{fix} it by removing all the frames pushed after the first continuation frame
of a linear fact. If we tried to use those frames, we would assumed that the linear facts used by the other frames were still in the database, but that
is not true because they have been consumed during the first application of the comprehension.
For example, if the body is $\bang\mathtt{a(X), b(X), c(X)}$ and the continuation stack has three frames (one per fact), we cannot backtrack to the frame of $\mathtt{c(X)}$
since at that point the matching process was assuming that the previous \texttt{b(X)} linear fact was still available.
Moreover, we also remove the consumed linear facts
from the frames of \texttt{b(X)} and $\bang$\texttt{a(X)} in order to make the stack fully consistent with the new database.
This is performed by rules using the $\strans$and $\dall$judgments.

We finally stop applying the comprehension when the continuation stack is empty ($\contc end$). 
Aggregates use the same mechanism as comprehensions, however we also need to keep track of the accumulated value.

\subsection{Soundness}

The soundness theorem proves that if a rule was successfully derived in the LLD semantics then it can also be derived in the HLD semantics. The completeness theorem cannot
be proven since LLD lacks the non-determinism inherent in HLD.

We prove that matching the body of a rule in LLD can also be performed in HLD (matching soundness) and that deriving the head of the rule is also sound (derivation soundness). The matching soundness uses induction on the size of the continuation frames, the size of the continuation stack and the size of terms to match to reconstitute a particular matching tree in HLD.

The derivation soundness lemma is trivial except for the case of comprehensions and aggregates. For such cases we use a modified version of the matching soundness theorem applied to the comprehension's body. It gives us $n$ $\mz$and $n$ $\dz$proofs (for maximality) that are used to rebuild the full derivation proof in HLD. This theorem is proved by
induction on the size of the continuation stack and continuation frames and uses lemmas that prove the correctness of the continuation stack after each application.\footnote{Details can be
found in \url{https://github.com/flavioc/formal-meld/blob/master/doc.pdf?raw=true}.}

\begin{comment}
\subsubsection{Soundness}

The soundness theorem proves that if a rule was successfully derived in the LLD semantics then it can also be derived in the HLD semantics. The completeness theorem cannot
be proven since LLD lacks the non-determinism inherent in HLD.

The first main lemma of the soundness proof proves that if we can match the body
of a rule at the low level then we can also match the rule in the high level system using the same database.

\begin{lemma}[Body Match]
Given a match $\mo \Gamma; \Delta_1, \Delta_2; \cdot; A; B; \cdot; R \rightarrow \Xi'; \Delta'; \Gamma'$ that is related to $A$, $\Delta_1, \Delta_2$ and $\Gamma$, we get either:

\begin{enumerate}
   \item $\cont \cdot; B; R; \Gamma; \Xi'; \Delta'; \Gamma'$;
   \item $\mz \Delta_2 \rightarrow A$ and $\mo \Gamma; \Delta_1; \Delta_2; \cdot; B; C'; R \rightarrow \Xi'; \Delta'; \Gamma'$ (related)
\end{enumerate}
\end{lemma}
\begin{proof}Use the main matching soundness theorem.
\end{proof}

When we say that a match is related to a term $A$ and a database $\Delta_1, \Delta_2, \Gamma$ we mean that
the matching judgment is related to the body $A$ of a rule and the initial database is $\Delta_1, \Delta_2, \Gamma$, where $\Gamma$ are
the persistent facts, $\Delta_2$ the linear facts consumed to match the body and $\Delta_1$ the remaining linear facts.
Moreover, the continuation stack is related to $A$ and to the database, therefore it is consistent in relation to them.

The body match lemma tells us that if we start a match of a body $A$ we will either fail (1) and need to try another rule in $R$ or we succeed by building the high level matching judgment $\mz \Delta_2 \rightarrow A$ (the soundness result) and reaching the end of the matching process $\mo \Gamma; \Delta_1; \Delta_2; \cdot; B; C'; R \rightarrow \Xi'; \Delta'; \Gamma'$.

This lemma uses a more complicated theorem that is recursively defined through judgments $\m{match}_1$ and $\m{cont}$ that use mutual induction on the
size of the continuation stack, the size of the remaining terms to match and also the size of alternatives at each continuation frame. We always ensure that the
continuation stack stays consistent at all times, when backtracking or moving forward. Theorem is presented in Appendix~\ref{main_soundness_theorem}.
 
The second stepping stone in the soundness proof is the derivation lemma. After we successfully match
the body of a rule, we need to prove that the derivation process (through judgments $\m{derive}_1$)
is also sound. This lemma is as follows:

\begin{lemma}[Derivation]
   If $\done \Gamma; \Delta; \Xi; \Gamma_1; \Delta_1; \Omega \rightarrow \Xi'; \Delta'; \Gamma'$ then $\dz \Gamma; \Delta; \Gamma_1; \Delta_1; \Omega \rightarrow \Xi'; \Delta'; \Gamma'$.
\end{lemma}
\begin{proof}
   Straightforward use of induction on $\Omega$ (the terms of the head) except for the sub-case of comprehensions and aggregates, where we need to use the comprehension and aggregate theorems to construct the derivation tree using $n$ applications of the corresponding expression.
\end{proof}

In the case of proving the soundness of comprehensions, we use a very identical theorem to the one used
to prove the body match soundness. However, in this case we need to reuse the continuation stack several
times (as many as many comprehensions can be applied). Using induction on the continuation stack, we get
$n$ (where $n \ge 0$) applications of the comprehension and $n \; \m{match}$ and $n \; \m{derive}$ judgments
that can be used to rebuild the derivation tree at the low level by using the $\dz \with L$, $\dz \with R$
and $\dz \lolli$ rules to fold and unfold the comprehension term. The theorem for aggregates works similarly.
\end{comment}
