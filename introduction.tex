
\begin{comment}
The last decade has seen a priority shift for processor companies. If clock frequency
was once the main metric for performance, today computing power is measured in number of
cores in a single chip.
For software developers and computer scientists, once focused in developing sequential programs,
newer hardware usually meant faster programs without any change to the source code. Today,
the free lunch is over. Multicore processors are now forcing the development of
new software methodologies that take advantage of increasing processing power through parallelism.
However, parallel programming is difficult, usually because programs are written
in imperative and stateful programming languages that make use of low level synchronization
primitives such as locks, mutexes and barriers. This tends to make the task of managing multithreaded
execution quite intricate and error-prone, resulting in race hazards and deadlocks.
In the future, \emph{many-core} processors will make this task look even more daunting.

Advances in network speed and bandwidth are making distributed computing
more appealing. For instance, \emph{cloud computing} is a new emerging paradigm that wants
to make every computer connected to the Internet as a client of a pool of computing power,
where data can be retrieved and computation performed. From the perspective of high performance
computing, the \emph{computer cluster} is a well established paradigm that uses fast local area
networks to improve performance and solve problems that would take a long time with a single computer.
\end{comment}

Developments in parallel and distributed programming have given birth to several programming models.
At the end of the spectrum are lower-level programming abstractions such as
\emph{message passing} (e.g., MPI~\cite{gabriel04-open-mpi}) and \emph{shared memory}
(e.g., Pthreads or OpenMP~\cite{Chapman-2007-UOP-1370966}).
While such abstractions are very expressive and enable the programmer to write very performant code,
they tend to be very hard to use and debug, due to synchronization problems, making it difficult to
prove the program's correctness. On the other hand, we have many declarative programming models
such as logic or functional programming
that can be run in parallel~\cite{Blelloch:1996:PPA:227234.227246} and tend to be easier to reason about.

Due to the popularity of social networks and the explosion of the content available in the World Wide Web, there has been
increased interest in running graph based algorithms in a distribution fashion. The sheer size of
the available datasets makes it impossible to run such algorithms in a single machine.
Most of the available frameworks developed for these kinds of algorithms are implemented as libraries on top
of imperative programming languages and thus require knowledge of both the library and the interface, making
the learning curve steep for unexperienced users. In turn, reasoning about the programs requires knowing how
the library schedules execution.

% Falar do datalog

We have designed a new logic programming language called LM (Linear Meld) that tackles the problems
of parallelism and distribution in graph based algorithms.
LM is based on the Meld language created by Ashley-Rollman et
al~\cite{ashley-rollman-iclp09,ashley-rollman-derosa-iros07wksp}, which
was designed to program massively distributed systems made of modular robots with a dynamic topology.
In Meld, the distribution of computation is done by first partitioning the program state across the robots
and then making computation local to the node.

LM differs from other logic programming languages such as Datalog or Prolog for three reasons. First, we integrate both classical logic and linear logic into the language, therefore allowing some facts to be retracted and asserted in a logical fashion. Second, unlike Prolog, LM is a bottom up logic programming language (similar to Datalog) since the
database is updated incrementally as rules are applied. Third, LM is a language created with parallelism and
distribution in mind, being specially suited to solve graph based algorithms like machine learning problems and
search algorithms, although it can be used to solve other kinds of problems.

In this paper we want to present the syntax and semantics of our language, explain how programs can be written
and what novel ideas LM brings to the table. We identify three key contributions of our work:

\begin{description}
   \item[Linear Logic:] We integrated linear logic into the original Meld so that program state can be encoded naturally.
   Meld started as a classical logic programming language where everything that is derived is true until the end
   of the execution. Linear logic turns some logical facts into resources that will be consumed when a rule is applied. In turn, this makes it possible to represent program state in a natural and declarative fashion.
   \item[Distribution:] LM programs are naturally concurrent because facts are partitioned by vertices of a graph data structure. While the original Meld sees this graph as a network of robots, we see each node as a logical part of the program (for example, in the graph coloring algorithm, the vertex is the node of the graph we want to color). This is made possible due to the restrictions on derivation rules, which are only allowed to use facts from a single vertex.
   \item[Semantics:] Starting from a fragment of linear logic used in LM, we formalized a high level dynamic semantics that is closely related to this fragment.
   We then designed a low level dynamic semantics that is a blueprint for a low level implementation of LM. It is very
   close to our implementation and helped us during development. We also explain how we proved the soundness of our low level semantics.
\end{description}

To realize LM, we have implemented a compiler and a virtual machine from scratch that executes LM programs on multicore machines
\footnote{Source code is available at \url{http://github.com/flavioc/meld}.}. We also have a preliminary version that runs on networks by
using OpenMPI as a communication layer. Our experimental results show that LM has good scalability.
Several interesting programs were implemented such as belief propagation~\cite{Gonzalez+al:aistats09paraml},
belief propagation with residual splash~\cite{Gonzalez+al:aistats09paraml}, PageRank, graph coloring,
N queens, shortest path~\cite{Dijkstra}, diameter estimation, map reduce, game of life, quick-sort, neural network training, among others.
While these results are evidence that LM is a promising language, this paper will only focus on the more formal aspects of our work.
We next present the related work and then we give an overview of our language and some concrete programs.
